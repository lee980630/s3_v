#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SFT Search+Vision í†µí•© ëŸ¬ë„ˆ
- ìš”êµ¬ì‚¬í•­ ë°˜ì˜:
  1) ê²€ìƒ‰ ì¿¼ë¦¬/ì‘ë‹µ í”„ë¡œí† ì½œ: generation.pyì˜ execute_predictions ë°©ì‹(POST, [{query,id}])
  2) ì´ë¯¸ì§€ ì „ì²˜ë¦¬/í¬ë¡­ ë° vision í† í° ì£¼ì… ê·œì¹™: generation.pyì˜ process_image / _process_next_obs íë¦„
  3) ì „ì²´ ë£¨í”„: SFT_test_pipeline.pyì˜ í‰ê°€ íë¦„(ìƒ˜í”Œ ë°˜ë³µ, ë¡œê·¸/ê²°ê³¼ íŒŒì¼ ìœ ì§€)
  4) ê²€ìƒ‰ ê²°ê³¼ ì„ íƒ ì •ì±…: ë‹¤ìˆ˜ ì´ë¯¸ì§€ ì¤‘ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©
  5) í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ìƒí•œ: ì •ì„ì  íŠ¸ë ì¼€ì´ì…˜
  6) ë©€í‹° GPU: ë°°ì¹˜ í¬ê¸°ê°€ num_gpusë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ì§€ ì•Šìœ¼ë©´ íŒ¨ë”© ìƒ˜í”Œ ì¶”ê°€
  7) ë¡œê·¸/ì‚°ì¶œë¬¼ ê²½ë¡œ: ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ íŒŒì¼ëª… ìœ ì§€(conversation_history.jsonl, results/sft_test_results.jsonl, ./data/image_crop)

ì£¼ì˜: ì•„ë˜ ì½”ë“œëŠ” ëª¨ë¸ í˜¸ì¶œë¶€ë¥¼ "ì–´ëŒ‘í„°"ë¡œ ë¶„ë¦¬í–ˆë‹¤. ë„¤ SFT ëª¨ë¸ì˜ generate/forward ì‹œê·¸ë‹ˆì²˜ì— ë§ê²Œ Adapter ë‚´ë¶€ 2ê°œ TODO ì§€ì ì„ ì±„ìš°ë©´ ë™ì‘í•œë‹¤.
"""
from __future__ import annotations
import os, json, math, time, shutil
from io import BytesIO
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple, Optional
from transformers import AutoProcessor
from transformers import Qwen2_5_VLForConditionalGeneration

import numpy as np
from PIL import Image
import requests
import torch

# =============================
# Configs
# =============================
@dataclass
class BridgeConfig:
    model_path: str
    query_file: str  # JSON or JSONL with {uid, query, ...}
    search_url: str  # POST, body: [{"query": str, "id": uid}]
    image_crop_dir: str = "./data/image_crop"
    results_dir: str = "./results"
    results_file: str = "sft_test_results.jsonl"
    history_file: str = "conversation_history.jsonl"
    max_turns: int = 10
    max_prompt_length: int = 8192
    num_gpus: int = 1
    http_timeout_sec: int = 20
    topk: int = 1  # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ 1ê°œë§Œ ì‚¬ìš©(ì²« ë²ˆì§¸)

# =============================
# Utilities (from generation.py semantics)
# =============================

def process_image(image, max_pixels: int = 2048 * 2048, min_pixels: int = 512 * 512):
    """generation.pyì˜ ì „ì²˜ë¦¬ ê·œì¹™ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜."""
    if isinstance(image, dict):
        image = Image.open(BytesIO(image['bytes']))
    elif isinstance(image, str):
        image = Image.open(image)

    if (image.width * image.height) > max_pixels:
        resize_factor = math.sqrt(max_pixels / (image.width * image.height))
        width, height = int(image.width * resize_factor), int(image.height * resize_factor)
        image = image.resize((width, height))

    if (image.width * image.height) < min_pixels:
        resize_factor = math.sqrt(min_pixels / (image.width * image.height))
        width, height = int(image.width * resize_factor), int(image.height * resize_factor)
        image = image.resize((width, height))

    if image.mode != 'RGB':
        image = image.convert('RGB')

    return image


def map_bbox_to_raw(display_w: int, display_h: int, raw_w: int, raw_h: int, bbox_xyxy_on_display: List[int], pad_eval: bool = False) -> List[int]:
    """generation.pyì˜ ì¢Œí‘œ ë³€í™˜ ë¡œì§ì„ ì¬í˜„(ê²€ì¦ ëª¨ë“œì¼ ë•Œ Â±28 íŒ¨ë”©ì„ ë‘˜ ìˆ˜ ìˆê²Œ ì˜µì…˜ ì œê³µ)."""
    x1, y1, x2, y2 = bbox_xyxy_on_display
    if pad_eval:
        x1 -= 28; y1 -= 28; x2 += 28; y2 += 28
    x1 = max(0, int(raw_w * x1 / display_w))
    y1 = max(0, int(raw_h * y1 / display_h))
    x2 = min(raw_w, int(raw_w * x2 / display_w))
    y2 = min(raw_h, int(raw_h * y2 / display_h))
    return [x1, y1, x2, y2]


def crop_and_preprocess(image_path: str, bbox_xyxy_on_display: Optional[List[int]] = None, pad_eval: bool = False):
    """ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì—´ê³  í•„ìš”í•˜ë©´ bboxë¡œ í¬ë¡­ â†’ generation.py ì „ì²˜ë¦¬."""
    raw = Image.open(image_path)
    if bbox_xyxy_on_display is not None:
        dw, dh = raw.size  # display=ì´ì „ ë‹¨ê³„ ì´ë¯¸ì§€ í¬ê¸°ë¼ê³  ê°€ì •(ê°„ë‹¨í™”)
        x1, y1, x2, y2 = map_bbox_to_raw(dw, dh, raw.width, raw.height, bbox_xyxy_on_display, pad_eval)
        raw = raw.crop((x1, y1, x2, y2))
    return process_image(raw, 512*28*28, 256*28*28)


# =============================
# Search client (POST [{query,id}])
# =============================
class SearchClient:
    def __init__(self, url: str, timeout_sec: int = 20):
        self.url = url
        self.timeout = timeout_sec

    def search(self, uid: Any, query: str) -> List[str]:
        """ì„œë²„ì— POSTë¡œ [{query,id}] ì „ì†¡, ì‘ë‹µì—ì„œ image_file ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ.
        ë°˜í™˜: ì´ë¯¸ì§€ ê²½ë¡œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸(ìš°ë¦° ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©)."""
        sid = str(uid)
        import re
        m = re.search(r"(\d+)$", sid)
        sid = m.group(1) if m else sid

        payload = [{"query": query, "id": sid}]  # â† ìˆ«ì idë§Œ ë³´ëƒ„
        resp = requests.post(self.url, json=payload, timeout=self.timeout,
                             headers={"Content-Type": "application/json"})
        resp.raise_for_status()
        data = resp.json()  # ê¸°ëŒ€: [{...}] í˜¹ì€ [...] í˜•íƒœ
        # ì‘ë‹µ í˜•íƒœ í‘œì¤€í™”
        images: List[str] = []
        if isinstance(data, list) and len(data) > 0:
            entry = data[0] if isinstance(data[0], dict) else {}
            # ì„œë²„ê°€ ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì¤„ ìˆ˜ ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì •ê·œí™”
            if isinstance(entry, dict):
                # ì¼€ì´ìŠ¤1: {"image_file": "..."}
                if 'image_file' in entry and isinstance(entry['image_file'], str):
                    images = [entry['image_file']]
                # ì¼€ì´ìŠ¤2: {"images": ["..."]}
                elif 'images' in entry and isinstance(entry['images'], list):
                    images = [str(p) for p in entry['images']]
        return images


# =============================
# Vision Adapter (processor/tokenizerì— ë§ì¶° ì…ë ¥ êµ¬ì„±)
# =============================
class VisionAdapter:
    def __init__(self, processor, image_token: str = "<|image_pad|>"):
        self.processor = processor
        self.image_token = image_token
        self.merge_size = getattr(getattr(processor, 'image_processor', processor), 'merge_size', 14)

    def build_mm_prompt_fragment(self, pil_list: List[Image.Image], paths: List[str]) -> Tuple[str, Dict[str, torch.Tensor]]:
        iproc = getattr(self.processor, 'image_processor', None)
        if iproc is None:
            raise RuntimeError("processor.image_processorê°€ í•„ìš”í•¨")
        image_inputs = iproc(pil_list, return_tensors='pt')
        image_grid_thw = image_inputs['image_grid_thw']
        merge_len = self.merge_size ** 2

        pieces = []
        for g in image_grid_thw:
            n_tokens = int(torch.prod(g).item() // merge_len)
            pieces.append(f"<|vision_start|>{self.image_token * n_tokens}<|vision_end|>")
        fragment = "".join(pieces)

        # ê²½ë¡œ ì •ë³´ë„ ê°™ì´ ì¶”ê°€
        image_inputs["paths"] = paths
        return fragment, image_inputs



# =============================
# Padding helper for multi-GPU generation
# =============================
class PaddingHelper:
    @staticmethod
    def pad_for_ngpus(input_ids: List[List[int]], num_gpus: int) -> Tuple[List[List[int]], int]:
        if num_gpus <= 1:
            return input_ids, 0
        bs = len(input_ids)
        rem = bs % num_gpus
        if rem == 0:
            return input_ids, 0
        pad_needed = num_gpus - rem
        pad_seq = input_ids[0][:] if bs > 0 else [151643]
        return input_ids + [pad_seq for _ in range(pad_needed)], pad_needed

    @staticmethod
    def trim_padded(outputs: List[str], pad_count: int) -> List[str]:
        if pad_count == 0:
            return outputs
        return outputs[:-pad_count]


# =============================
# Model Adapter (fill the TODO for your SFT model)
# =============================
class ModelAdapter:
    def __init__(self, model_path: str):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self._load(model_path)

    def _load(self, model_path: str):
        # processorì™€ ëª¨ë¸ ë¡œë“œ
        print(f"Loading model from: {model_path}")
        self.processor = AutoProcessor.from_pretrained(model_path)
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.tokenizer = self.processor.tokenizer
        self.model.eval()
        print("Model loaded successfully.")

    @torch.inference_mode()
    def generate_once(
        self,
        messages: List[Dict[str, str]],
        image_inputs: Optional[Dict[str, Any]] = None,
        max_new_tokens: int = 512
    ) -> str:
        #1. ë©”ì‹œì§€ë¥¼ chat templateë¡œ ì§ë ¬í™”
        # prompt = self.processor.tokenizer.apply_chat_template(
        #     messages, tokenize=False, add_generation_prompt=True
        # )

        prompt = "".join([f"{m['role']}:\n{m['content']}\n" for m in messages])

        # 2. ì´ë¯¸ì§€ ì²˜ë¦¬ (ìˆì„ ê²½ìš°)
        raw_images = None
        if image_inputs and "paths" in image_inputs:
            raw_images = [Image.open(p).convert("RGB") for p in image_inputs["paths"]]

        # 3. processorë¡œ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ì¸ì½”ë”©
        inputs = self.processor(
            text=prompt, images=raw_images, return_tensors="pt"
        ).to(self.model.device)

        #ë””ë²„ê¹…
        # print("[DEBUG] prompt:", prompt[:500], "...")
        # print("[DEBUG] raw_images:", [img.size for img in (raw_images or [])])
        # print("[DEBUG] inputs keys:", inputs.keys())
        # for k, v in inputs.items():
        #     if hasattr(v, "shape"):
        #         print("  ", k, v.shape, v.device)
        ##

        # 4. ëª¨ë¸ë¡œ generate ì‹¤í–‰
        outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)

        # 5. ì¶œë ¥ ë””ì½”ë”©
        response_text = self.processor.decode(outputs[0], skip_special_tokens=True)
        print("------------ëŒ€ë‹µğŸš€------------ :\n", response_text)

        return response_text


# =============================
# Runner (SFT_test_pipeline ìŠ¤íƒ€ì¼ ë£¨í”„)
# =============================
class SFTRunner:
    def __init__(self, cfg: BridgeConfig):
        self.cfg = cfg
        os.makedirs(self.cfg.image_crop_dir, exist_ok=True)
        os.makedirs(self.cfg.results_dir, exist_ok=True)
        self.search = SearchClient(cfg.search_url, cfg.http_timeout_sec)
        self.model = ModelAdapter(cfg.model_path)
        #self.vision = (self.model.processor)
        self.vision = VisionAdapter(self.model.processor)  # â˜… ìˆ˜ì •


    def _append_jsonl(self, path: str, obj: Dict[str, Any]):
        with open(path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")

    def _load_queries(self) -> List[Dict[str, Any]]:
        with open(self.cfg.query_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # "examples" í‚¤ê°€ ìˆìœ¼ë©´ ê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
        if isinstance(data, dict) and "examples" in data:
            return data["examples"]

        # ë¦¬ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if isinstance(data, list):
            return data

        # ë‹¨ì¼ ê°ì²´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë°˜í™˜
        return [data]


    def _parse_action(self, text: str) -> Tuple[str, Optional[str]]:
        """ëª¨ë¸ ì¶œë ¥ì—ì„œ <search>/<bbox>/<answer> ì¤‘ í•˜ë‚˜ë¥¼ ì¶”ì¶œ."""
        import re
        m = re.search(r"<(search|bbox|answer)>(.*?)</\1>", text, flags=re.DOTALL)
        if not m:
            return "", None
        return m.group(1), m.group(2).strip()

    # def _messages_with_image(self, base_messages: List[Dict[str,str]], pil_images: List[Image.Image]) -> Tuple[List[Dict[str,str]], Dict[str, torch.Tensor]]:
    #     fragment, image_inputs = self.vision.build_mm_prompt_fragment(pil_images)
    #     # ë§ˆì§€ë§‰ user turn ë’¤ì— vision fragmentë¥¼ ë¶™ì´ëŠ” ê°„ë‹¨í•œ êµ¬í˜„
    #     msgs = base_messages[:]
    #     if msgs and msgs[-1]['role'] == 'user':
    #         msgs[-1] = {**msgs[-1], 'content': msgs[-1]['content'] + "\n" + fragment}
    #     else:
    #         msgs.append({"role": "user", "content": fragment})
    #     return msgs, image_inputs
    def _messages_with_image(
        self,
        base_messages: List[Dict[str,str]],
        pil_images: List[Image.Image],
        paths: List[str],                          # â˜… ì¶”ê°€
    ) -> Tuple[List[Dict[str,str]], Dict[str, torch.Tensor]]:
        fragment, image_inputs = self.vision.build_mm_prompt_fragment(pil_images, paths)  # â˜… paths ì „ë‹¬
        msgs = base_messages[:]
        if msgs and msgs[-1]['role'] == 'user':
            msgs[-1] = {**msgs[-1], 'content': msgs[-1]['content'] + "\n" + fragment}
        else:
            msgs.append({"role": "user", "content": fragment})
        #ë””ë²„ê·¸
        # print("[DEBUG] vision fragment:", fragment[:120], "...")
        # print("[DEBUG] image_inputs keys:", list(image_inputs.keys()))
        # if "paths" in image_inputs:
        #     print("[DEBUG] image paths:", image_inputs["paths"])
        #
        
        return msgs, image_inputs

    def run_one_sample(self, item: Dict[str, Any]) -> Dict[str, Any]:
        uid = item.get('uid') or item.get('id') or str(item.get('uid', '0'))
        query = item['query'] if 'query' in item else item.get('question', '')
        history_path = os.path.join(self.cfg.results_dir, self.cfg.history_file)
        result_path = os.path.join(self.cfg.results_dir, self.cfg.results_file)

        system_prompt = (
            "Answer the given question.\n"
            "You must conduct reasoning inside <think> and </think> every time you get new information.\n"
            "After reasoning, if you find you lack some knowledge, you can call a search engine using <search> query </search>.\n"
            "Whenever you retrieve an image, you may crop it for a clearer view using <bbox>[x1, y1, x2, y2]</bbox>.\n"
            "If you determine that no further external knowledge is needed, you must finish with <search_complete>true</search_complete>."
        )
        messages = [
            {
                "role": "user",
                "content": f"{system_prompt}\n\nQuestion: {query}"
            }
        ]

        convo_log: List[Dict[str, Any]] = []
        retrieved_images: List[str] = []

        for step in range(self.cfg.max_turns):
            # 1) ëª¨ë¸ í•œ í„´ ìƒì„±
            text = self.model.generate_once(messages)
            convo_log.append({"role": "assistant", "content": text})
            self._append_jsonl(history_path, {"uid": uid, "step": step, "assistant": text})

            # act, content = self._parse_action(text)
            # if act == 'answer' or 'search_complete' in text:
            #     # ì¢…ë£Œ
            #     out = {"uid": uid, "status": "success", "answer": text, "images": retrieved_images}
            #     self._append_jsonl(result_path, out)
            #     return out
            act, content = self._parse_action(text)
            print("[DEBUG] PARSED ACT:", act, "| CONTENT:", repr(content))  # â† ì¶”ê°€ ë””ë²„ê¹…
            if act in ("answer", "search_complete"):
                out = {"uid": uid, "status": "success", "answer": text, "images": retrieved_images}
                self._append_jsonl(result_path, out)
                return out


            if act == 'search' and content:
                # 2) ê²€ìƒ‰ â†’ ì²« ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
                try:
                    print("[DEBUG] CALL SEARCH:", self.cfg.search_url, "uid=", uid, "q=", content)  # â† ì¶”ê°€ ë””ë²„ê¹…
                    images = self.search.search(uid, content)
                except Exception as e:
                    out = {"uid": uid, "status": "failed_search", "error": str(e)}
                    self._append_jsonl(result_path, out)
                    return out
                if not images:
                    # ê²€ìƒ‰ ì‹¤íŒ¨ â†’ íŒíŠ¸ ì£¼ê³  ë‹¤ìŒ í„´ ìœ ë„
                    messages.append({"role": "user", "content": "<information></information>"})
                    continue
                img_path = images[0]
                retrieved_images.append(img_path)

                # 3) ì „ì²˜ë¦¬í•˜ì—¬ vision fragment + image_inputs ì¤€ë¹„
                try:
                    pil = process_image(img_path, 512*28*28, 256*28*28)
                except Exception:
                    # ì—´ ìˆ˜ ì—†ëŠ” ê²½ë¡œë©´ ìŠ¤í‚µ
                    messages.append({"role": "user", "content": "<information></information>"})
                    continue
                #messages, image_inputs = self._messages_with_image(messages, [pil])
                messages, image_inputs = self._messages_with_image(messages, [pil], [img_path])  # â˜… paths í•¨ê»˜
                # ë‹¤ìŒ í„´ì—ì„œ ëª¨ë¸ì´ bboxë¥¼ ë‚¼ ìˆ˜ ìˆë„ë¡ ê·¸ëŒ€ë¡œ ì§„í–‰
                continue

            if act == 'bbox' and content:
                # 4) bbox í¬ë¡­(ê°€ì¥ ë§ˆì§€ë§‰ retrieval ì´ë¯¸ì§€ ëŒ€ìƒìœ¼ë¡œ)
                if not retrieved_images:
                    messages.append({"role": "user", "content": "Your bbox is invalid without an image. Try search first."})
                    continue
                try:
                    xyxy = json.loads(content)
                except Exception:
                    messages.append({"role": "user", "content": "Your bbox is invalid JSON. Try again."})
                    continue
                # ë””ìŠ¤í”Œë ˆì´=ì›ë³¸ ê°€ì •(ê°„ë‹¨í™”)
                cropped = crop_and_preprocess(retrieved_images[-1], xyxy, pad_eval=False)
                # ì €ì¥(ê´€ì°°ì„±)
                save_name = f"crop_{uid}_{step}.jpg"
                save_path = os.path.join(self.cfg.image_crop_dir, save_name)
                cropped.save(save_path)
                # ë‹¤ìŒ í„´ ì…ë ¥ìœ¼ë¡œ í¬ë¡­ ì´ë¯¸ì§€ ë¶€ì°©
                messages, image_inputs = self._messages_with_image(messages, [cropped])
                continue

            # 5) ê¸°íƒ€ â†’ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„ ìœ ë„
            messages.append({"role": "user", "content": (
                "Your previous action is invalid.\n"
                "Think first inside <think></think>, then use <search> or <bbox> or <search_complete>"
            )})

        out = {"uid": uid, "status": "max_turn_reached", "images": retrieved_images}
        self._append_jsonl(result_path, out)
        return out

    def eval_all(self):
        data = self._load_queries()
        ok, fail = 0, 0
        for item in data:
            res = self.run_one_sample(item)
            if res.get('status') == 'success':
                ok += 1
            else:
                fail += 1
        print(f"Done. success={ok}, fail={fail}")


# =============================
# CLI
# =============================
if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument('--model_path', required=True)
    p.add_argument('--query_file', required=True)
    p.add_argument('--search_url', required=True)
    p.add_argument('--results_dir', default='./results')
    p.add_argument('--image_crop_dir', default='./data/image_crop')
    p.add_argument('--max_turns', type=int, default=10)
    p.add_argument('--max_prompt_length', type=int, default=8192)
    p.add_argument('--num_gpus', type=int, default=1)
    p.add_argument('--http_timeout_sec', type=int, default=20)
    args = p.parse_args()

    cfg = BridgeConfig(
        model_path=args.model_path,
        query_file=args.query_file,
        search_url=args.search_url,
        results_dir=args.results_dir,
        image_crop_dir=args.image_crop_dir,
        max_turns=args.max_turns,
        max_prompt_length=args.max_prompt_length,
        num_gpus=args.num_gpus,
        http_timeout_sec=args.http_timeout_sec,
    )

    runner = SFTRunner(cfg)
    runner.eval_all()
