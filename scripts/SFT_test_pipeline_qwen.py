import os
import json
from tqdm import tqdm
import re
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from llama_index.core.schema import NodeWithScore, ImageNode
import sys
import time
import requests
from typing import List, Dict, Any, Optional, Tuple
import zlib  # uid í•´ì‹œ fallbackìš©

# --- âœ¨ 1ë‹¨ê³„ ìˆ˜ì •: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ âœ¨ ---
import torch
from transformers import AutoProcessor
from transformers import Qwen2_5_VLForConditionalGeneration
from PIL import Image # ì´ë¯¸ì§€ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ PILë„ import í•©ë‹ˆë‹¤.

# (ê¸°ì¡´ ì „ì—­ ë³€ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
image_output_dir = './data/image_crop'
raw_image_dir = './search_engine/corpus/img'

# Dashscope ê´€ë ¨ ì„¤ì •ì€ ì´ì œ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ë‹¤ë¥¸ ê¸°ëŠ¥ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ë‚¨ê²¨ë‘˜ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# import dashscope
# from http import HTTPStatus
# dashscope.base_http_api_url = ...
# API_KEY = ...
# dashscope.api_key = API_KEY

search_engine_url = "http://163.239.28.21:5002/search"
TOPK = 10

# prompt_inst ì™€ prompt_user_start ëŠ” ì‚­ì œ ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬

# USER_PROMPT = '''You are a search agent.
# You must conduct reasoning inside <think> and </think> every time you get new information. 
# After reasoning, if you find you lack some knowledge, you can call a search engine using <search> query </search> and the user will return the search results. 
# Whenever you retrieve an image, you may crop it for a clearer view using <bbox>[x1, y1, x2, y2]</bbox>. 
# You can search as many times as you want. 
# If you determine that no further external knowledge is needed, you must finish with <search_complete>true</search_complete>. 
# Otherwise, continue with <search> or <bbox> actions until you are ready to finish. Question: {question}'''
#USER_PROMPT = '''First, think step-by-step about the user's question inside <think> tags. Then, perform an action like <search> or <bbox> or <search_complete>.
#Question: {question}
#'''
# (ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì€ ìˆ˜ì • ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)

# SYSTEM_RULES = (
#     "You are a search agent.\n"
#     "You must always begin with <think>...</think> showing your reasoning about the question.\n"
#     "After reasoning, output exactly one action tag among <search>...</search> or <bbox>[x1, y1, x2, y2]</bbox> or <search_complete>true</search_complete>.\n"
#     "Do not write anything before <think>. Keep actions on a new line after </think>."
# )

SYSTEM_RULES = (
"You are a search agent.\n"
"You must conduct reasoning inside <think> and </think> every time you get new information.\n"
"After reasoning, if you find you lack some knowledge, you can call a search engine using <search> query </search> and the user will return the search results. \n"
"Whenever you retrieve an image, you may crop it for a clearer view using <bbox>[x1, y1, x2, y2]</bbox>. /n"
"You can search as many times as you want. /n"
"If you determine that no further external knowledge is needed, you must finish with <search_complete>true</search_complete>.\n"
"Otherwise, continue with <search> or <bbox> actions until you are ready to finish.\n"
)

USER_QUESTION_FMT = (
    "Question: {question}"
)


import re
import ast 
def process_image(image, max_pixels: int = 2048 * 2048, min_pixels: int = 512 * 512):
    import math

    if isinstance(image, str):
        image = Image.open(image)
    if (image.width * image.height) > max_pixels:
        resize_factor = math.sqrt(max_pixels / (image.width * image.height))
        image = image.resize((int(image.width * resize_factor), int(image.height * resize_factor)))
    if (image.width * image.height) < min_pixels:
        resize_factor = math.sqrt(min_pixels / (image.width * image.height))
        image = image.resize((int(image.width * resize_factor), int(image.height * resize_factor)))
    if image.mode != 'RGB':
        image = image.convert('RGB')
    return image

def _sanitize_for_json(obj):
    # dict, list ì¬ê·€ ì²˜ë¦¬
    if isinstance(obj, dict):
        return {k: _sanitize_for_json(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_sanitize_for_json(x) for x in obj]
    # PIL Image â†’ ë¬¸ìì—´ë¡œ ì¹˜í™˜
    try:
        from PIL.Image import Image as PILImage
        if isinstance(obj, PILImage):
            return "<PIL.Image omitted>"
    except Exception:
        pass
    return obj




def extract_json(response: str) -> dict:
    """
    <think>, <search>, <bbox>, <search_complete> íƒœê·¸ë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ì—ì„œ
    ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    think_content = re.search(r"<think>(.*?)</think>", response, re.DOTALL)
    search_content = re.search(r"<search>(.*?)</search>", response, re.DOTALL)
    bbox_content = re.search(r"<bbox>(.*?)</bbox>", response, re.DOTALL)
    search_complete_found = "<search_complete>true</search_complete>" in response

    result_json = {}

    if think_content:
        result_json["think"] = think_content.group(1).strip()
    
    if search_content:
        result_json["search"] = search_content.group(1).strip()
    
    if bbox_content:
        bbox_str = bbox_content.group(1).strip()
        try:
            result_json["bbox"] = ast.literal_eval(bbox_str)
        except (ValueError, SyntaxError):
            print(f"Warning: Could not parse bbox string: {bbox_str}")
            result_json["bbox"] = bbox_str

    if search_complete_found:
        result_json["search_complete"] = True

    if not result_json or "think" not in result_json:
        raise ValueError("A valid action with a <think> tag was not found in the response.")

    return result_json
def crop_and_dump(image_path, bbox, output_folder=image_output_dir):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    try:
        image = Image.open(image_path)
    except Exception as e:
        print(f"cannot open {image_path}: {e}")
        return None
    x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]
    cropped_image = image.crop((x1, y1, x2, y2))
    timestamp = int(time.time() * 1000)
    file_extension = os.path.splitext(image_path)[1]
    output_filename = f"{timestamp}{file_extension}"
    output_path = os.path.join(output_folder, output_filename)
    print(image_path)
    print(output_path)
    try:
        cropped_image.save(output_path)
        return output_path
    except Exception as e:
        print(f"fail: {e}")
        return None

def _uid_to_query_id(uid: str | int | None) -> int:
    if uid is None: return 0
    if isinstance(uid, int): return uid
    s = str(uid)
    m = re.search(r'(\d+)$', s)
    if m:
        try: return int(m.group(1))
        except: pass
    return zlib.crc32(s.encode("utf-8")) & 0xffffffff

class HTTPImageSearcher:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.session = requests.Session()

    def query(self, query: str, query_id: str | int, topk: int = TOPK) -> List[str]:
        try: query_id = int(query_id)
        except Exception: query_id = _uid_to_query_id(query_id)

        batch_reqs = [{"query": query, "id": str(query_id)}]

        try:
            # POST ë°©ì‹ìœ¼ë¡œ jsonì„ ì‚¬ìš©í•´ ë°ì´í„° ì „ì†¡
            resp = self.session.post(
                self.base_url,
                json=batch_reqs, # 'params' ëŒ€ì‹  'json' ì‚¬ìš©
                timeout=20
            )
            resp.raise_for_status()
            data = resp.json()
            batch0 = data[0] if isinstance(data, list) and data else []
            paths = [item["image_file"] for item in batch0 if "image_file" in item]
            return paths[:topk]
        except Exception as e:
            print(f"[SEARCH ERROR] {e}")
            return []


# --- âœ¨ 1ë‹¨ê³„ ìˆ˜ì •: SFT ëª¨ë¸ì„ ìœ„í•œ ìƒˆë¡œìš´ 'ì—”ì§„' í´ë˜ìŠ¤ êµ¬í˜„ âœ¨ ---
class SFT_VLM_Role:
    """ë¡œì»¬ SFT ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, model_path: str):
        print(f"Loading model from: {model_path}")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = AutoProcessor.from_pretrained(model_path)
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )

        print("Model loaded successfully.")

# í…ìŠ¤íŠ¸(messages)ì™€ í•¨ê»˜ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸(image_paths)ë„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
    def generate(self, messages, image_paths=None):
        try:
            # (A) í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ìƒì„±: í† í¬ë‚˜ì´ì €ì˜ chat_template ì‚¬ìš©
            prompt = self.processor.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            # (B) ì´ë¯¸ì§€ ë¡œë”© (ìˆìœ¼ë©´)
            raw_images = [Image.open(p).convert("RGB") for p in image_paths] if image_paths else None

            # (C) í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ë¥¼ processorë¡œ í•œë²ˆì— í…ì„œí™”
            inputs = self.processor(
                text=prompt,
                images=raw_images,
                return_tensors="pt"
            ).to(self.model.device)

            # (D) ìƒì„±
            outputs = self.model.generate(**inputs, max_new_tokens=512)
            prompt_len = inputs['input_ids'].shape[-1] if 'input_ids' in inputs else 0
            gen_ids = outputs[0, prompt_len:] if prompt_len > 0 else outputs[0]

            # ë¹ˆ ìƒì„±(ëª¨ë¸ì´ ì•„ë¬´ ê²ƒë„ ì•ˆ ëƒˆì„ ë•Œ) ëŒ€ë¹„
            if gen_ids.numel() == 0:
                return ""

            # tokenizerë¡œ ë””ì½”ë“œ (processor.batch_decode ë§ê³ )
            generated = self.processor.tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

            return generated

        except Exception as e:
            print(f"[SFT VLM Generate Error] {e}")
            return None



class MMRAG:
    def __init__(self,
                model_path: str,
                dataset='search_engine/corpus',
                query_file='rag_dataset.json',
                experiment_type = 'cot',
                workers_num = 1,
                topk=TOPK):
        
        # --- __init__ í•¨ìˆ˜ì˜ ë‚´ìš©ì€ ëŒ€ë¶€ë¶„ ë™ì¼ ---
        self.experiment_type = experiment_type
        self.workers_num = workers_num
        self.top_k = topk
        self.dataset = dataset
        self.query_file = query_file
        self.dataset_dir = os.path.join('./', dataset)
        self.img_dir = os.path.join(self.dataset_dir, "img")
        self.results_dir = os.path.join(self.dataset_dir, "results")
        os.makedirs(self.results_dir, exist_ok=True)

        if experiment_type == 'cot':
            self.eval_func = self.cot_collect
            self.output_file_name = 'sft_test_results.jsonl' 

        self.output_file_path = os.path.join(self.results_dir, self.output_file_name.replace("/","-"))
        self.conversation_log_path = os.path.join(self.results_dir, "conversation_history.jsonl")
        
        self.sft_model = SFT_VLM_Role(model_path=model_path)
        self.searcher = HTTPImageSearcher(search_engine_url)

    # --- âœ¨ 1. postprocess_predictions í•¨ìˆ˜ ì¶”ê°€ âœ¨ ---
    def postprocess_predictions(self, prediction: str) -> Tuple[str, str]:
        """ëª¨ë¸ì˜ ì›ë³¸ ì‘ë‹µì—ì„œ ì•¡ì…˜ê³¼ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        pattern = r'<(search|bbox|answer|search_complete)>(.*?)</\1>'
        match = re.search(pattern, prediction, re.DOTALL)
        if match:
            action = match.group(1)
            content = match.group(2).strip()
            # search_completeëŠ” ë‚´ìš©ì´ ì—†ìœ¼ë¯€ë¡œ contentë¥¼ Trueë¡œ ì„¤ì •
            if action == 'search_complete':
                content = True
            return action, content
        return None, prediction # ìœ íš¨í•œ íƒœê·¸ê°€ ì—†ìœ¼ë©´ actionì€ None, contentëŠ” ì›ë³¸ ì‘ë‹µ

    # --- âœ¨ 2. execute_predictions í•¨ìˆ˜ ì¶”ê°€ âœ¨ ---
    def execute_predictions(self, action: str, content: Any, uid: str, all_images: List[str]) -> Tuple[Any, bool]:
        """ì¶”ì¶œëœ ì•¡ì…˜ì„ ì‹¤í–‰í•˜ê³ , ë‹¤ìŒ observationê³¼ ì¢…ë£Œ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        next_obs = None
        done = False

        if action == 'search':
            query_id = _uid_to_query_id(uid)
            # HTTPImageSearcherë¥¼ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤.
            search_results = self.searcher.query(content, query_id, self.top_k)
            next_obs = search_results
        
        elif action == 'bbox':
            if len(all_images) > 0:
                try:
                    bbox_value = ast.literal_eval(content) if isinstance(content, str) else content
                    if len(bbox_value) == 4 and all(isinstance(x, (int, float)) for x in bbox_value):
                         # crop_and_dump í•¨ìˆ˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                        next_obs = crop_and_dump(all_images[-1], bbox_value)
                    else:
                        raise ValueError("Invalid bbox value")
                except:
                    next_obs = "Your previous bbox action was invalid. Please try again."
            else:
                next_obs = "You tried to crop, but there are no images retrieved yet."

        elif action == 'answer' or action == 'search_complete':
            done = True
        
        else: # ìœ íš¨í•œ ì•¡ì…˜ íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°
            next_obs = "Your response did not contain a valid action tag. Please use <search>, <bbox>, or <search_complete>."

        return next_obs, done

    # --- âœ¨ 3. cot_collect í•¨ìˆ˜ ë‹¨ìˆœí™” âœ¨ ---
    def cot_collect(self, sample):
        query = sample['query']
        messages = [
            {"role": "system", "content": SYSTEM_RULES},
            {"role": "user",   "content": USER_QUESTION_FMT.format(question=query)}
        ]
            
        all_images = []
        crop_images = []
        status = "failed_unknown"
        final_result = None
        raw_response = ""

        try:
            for _ in range(10): # ìµœëŒ€ 10í„´
                # response = self.sft_model.generate(
                #     messages=messages, 
                #     image_paths=[all_images[-1]] if all_images else None
                # )
                response = self.sft_model.generate(messages=messages)

                raw_response = response
                if response is None:
                    status = "failed_model_none_response"; break
                
                print("------------ëŒ€ë‹µğŸš€------------ :\n",response)

                # 1. ëª¨ë¸ ì‘ë‹µì—ì„œ ì•¡ì…˜ê³¼ ë‚´ìš© ì¶”ì¶œ
                action, content = self.postprocess_predictions(response)
                
                # 2. ì¶”ì¶œëœ ì•¡ì…˜ ì‹¤í–‰
                next_obs, done = self.execute_predictions(action, content, sample.get('uid'), all_images)

                # 3. ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
                messages.append({"role": "assistant", "content": response}) # ëª¨ë¸ì˜ ì›ë³¸ ì‘ë‹µì„ ê¸°ë¡
                '''
                if done:
                    status = "success"
                    sample['collected'] = {"images": all_images, "crops": crop_images}
                    final_result = sample
                    break
                '''
                if done:
                    status = "success"
                    final_result = {
                        "uid": sample.get("uid"),
                        "query": query,
                        "collected": {
                            "images": all_images,          # ë¬¸ìì—´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
                            "crops": crop_images           # ë¬¸ìì—´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
                        },
                        # í˜¹ì‹œ ëª¨ë¥¼ ë¹„ì§ë ¬í™” ì”ì¬ ëŒ€ë¹„ ìŠµê´€ì ìœ¼ë¡œ sanitize
                        "history": _sanitize_for_json(messages)
                    }
                    break


                # 4. ë‹¤ìŒ ì…ë ¥ì„ ìœ„í•œ observation ì²˜ë¦¬
                if isinstance(next_obs, str): # ì—ëŸ¬ ë©”ì‹œì§€ë‚˜ í…ìŠ¤íŠ¸ ì •ë³´
                    messages.append({"role": "user", "content": next_obs})
                elif isinstance(next_obs, list) and len(next_obs) > 0:  # ê²€ìƒ‰ ê²°ê³¼ (ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸)
                    selected_image = next_obs[0]
                    all_images.append(selected_image)

                    # â›” ì´ë¯¸ì§€ ì—´ì§€ ì•ŠìŒ, í…ì„œ ìƒì„±í•˜ì§€ ì•ŠìŒ, vision token ë§Œë“¤ì§€ ì•ŠìŒ
                    # âœ… ë¡œê·¸ìš©ìœ¼ë¡œ "ê²½ë¡œ ë¬¸ìì—´"ë§Œ ë©”ì‹œì§€ì— ë‚¨ê¹€
                    messages.append({
                        "role": "user",
                        "content": f"[retrieved_image_path]: {selected_image}"
                    })
# '''
#                     selected_image = next_obs[0]
#                     all_images.append(selected_image)

#                     # --- â‘  ì´ë¯¸ì§€ ë¡œë“œ + ì „ì²˜ë¦¬ ---
#                     raw_img = process_image(selected_image)

#                     # --- â‘¡ tensor ë³€í™˜ ---
#                     image_inputs = self.sft_model.processor.image_processor([raw_img], return_tensors="pt")
#                     image_grid_thw = image_inputs['image_grid_thw']

#                     # --- â‘¢ vision í† í° ë¬¸ìì—´ ìƒì„± ---
#                     merge_length = self.sft_model.processor.image_processor.merge_size**2
#                     vision_tokens = ''.join([
#                         f"<|vision_start|>{self.sft_model.processor.image_token * (grid.prod() // merge_length)}<|vision_end|>"
#                         for grid in image_grid_thw
#                     ])

#                     # --- â‘£ user role ë©”ì‹œì§€ë¡œ ì‚½ì… ---
#                     messages.append({"role": "user", "content": vision_tokens})
#                     messages.append({
#                         "role": "user",
#                         "content": [
#                             {"type": "image", "image": Image.open(selected_image).convert("RGB")},
#                             {"type": "text", "text": "Here is the retrieved image."}
#                         ]
#                     })
#                     # --- â‘¤ tensorëŠ” ë³„ë„ë¡œ ì €ì¥ (ëª¨ë¸ forward ì‹œ ê°™ì´ ì „ë‹¬í•˜ê¸° ìœ„í•¨) ---
#                     if "multi_modal_inputs" not in sample:
#                         sample["multi_modal_inputs"] = []
#                     sample["multi_modal_inputs"].append(image_inputs)
# '''
# '''
#                 elif isinstance(next_obs, str) and next_obs.endswith(('.png', '.jpg', '.jpeg')):  # í¬ë¡­ ê²°ê³¼
#                     crop_img = process_image(next_obs)
#                     image_inputs = self.sft_model.processor.image_processor([crop_img], return_tensors="pt")

#                     merge_length = self.sft_model.processor.image_processor.merge_size**2
#                     vision_tokens = ''.join([
#                         f"<|vision_start|>{self.sft_model.processor.image_token * (grid.prod() // merge_length)}<|vision_end|>"
#                         for grid in image_inputs['image_grid_thw']
#                     ])

#                     # user role ë©”ì‹œì§€ ì¶”ê°€
#                     messages.append({"role": "user", "content": vision_tokens})

#                     # tensor ë³´ê´€
#                     if "multi_modal_inputs" not in sample:
#                         sample["multi_modal_inputs"] = []
#                     sample["multi_modal_inputs"].append(image_inputs)

#                     # crop ì´ë¯¸ì§€ ê²½ë¡œë„ ê¸°ë¡
#                     crop_images.append(next_obs)
# '''

                elif isinstance(next_obs, str) and next_obs.endswith(('.png', '.jpg', '.jpeg')):  # í¬ë¡­ ê²°ê³¼
                    # â›” ì´ë¯¸ì§€ ì—´ì§€ ì•ŠìŒ, í…ì„œ ìƒì„±í•˜ì§€ ì•ŠìŒ, vision token ë§Œë“¤ì§€ ì•ŠìŒ
                    # âœ… ë¡œê·¸ì— "ê²½ë¡œ ë¬¸ìì—´"ë§Œ ê¸°ë¡
                    messages.append({"role": "user", "content": f"[crop_image_path]: {next_obs}"})
                    crop_images.append(next_obs)
                

            if not final_result:
                status = "failed_timeout"

        except Exception as e:
            print(f"Processing failed for UID {sample.get('uid')}: {e}\n---")
            status = "failed_exception"
        
        finally:
    
            # --- ë¡œê¹…ì€ ì—¬ê¸°ì„œ ë”± í•œ ë²ˆë§Œ ìˆ˜í–‰ ---
            safe_messages = _sanitize_for_json(messages)

            with open(self.conversation_log_path, "a", encoding="utf-8") as f:
                log_entry = {
                    "uid": sample.get('uid'), "status": status,
                    "raw_response_on_fail": raw_response if "fail" in status else None,
                    "conversation": safe_messages
                }
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

            
            if final_result:
                final_result['history'] = messages
                final_result['recall_results'] = dict(
                    source_nodes=[NodeWithScore(node=ImageNode(image_path=img), score=None).to_dict() for img in all_images],
                    response=None, metadata=None)
            
            return final_result


    # (eval_dataset í•¨ìˆ˜ëŠ” ìˆ˜ì • ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    def eval_dataset(self):
        eval_func = self.eval_func
        rag_dataset_path = os.path.join(self.dataset_dir,self.query_file)
        with open(rag_dataset_path, "r") as f: data = json.load(f)
        data = data['examples']
        if os.path.exists(self.output_file_path):
            results = []
            with open(self.output_file_path, "r") as f:
                for line in f:
                    try: results.append(json.loads(line.strip()))
                    except: continue
            uid_already = set()
            for item in results:
                if isinstance(item, dict) and 'uid' in item: uid_already.add(item['uid'])
            data = [item for item in data if item.get('uid') not in uid_already]
        if self.workers_num == 1:
            for item in tqdm(data):
                result = eval_func(item)
                if result is None: continue
                with open(self.output_file_path, "a", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False)
                    f.write("\n")
        else:
            from threading import Lock
            write_lock = Lock()
            with ThreadPoolExecutor(max_workers=self.workers_num) as executor:
                futures = [executor.submit(eval_func, item) for item in data]
                for future in tqdm(as_completed(futures), total=len(futures), desc="Processing"):
                    try: result = future.result()
                    except Exception as e:
                        print(f"[WORKER ERROR] {e}")
                        continue
                    if result is None: continue
                    with write_lock:
                        with open(self.output_file_path, "a", encoding="utf-8") as f:
                            f.write(json.dumps(result, ensure_ascii=False) + "\n")
        return self.output_file_path

def arg_parse():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str,default ='./model/global_step_444/global_step_444', required=True, help="Path to the local SFT model") #ì¶”ê°€
    parser.add_argument("--dataset", type=str, default='example', help="The name of dataset")
    parser.add_argument("--query_file", type=str, default='./data/test_dataset/test_dataset.json', help="The name of anno_file")
    parser.add_argument("--experiment_type", type=str, default='cot', help="The type of experiment")
    parser.add_argument("--workers_num", type=int, default=10, help="The number of workers")
    parser.add_argument("--topk", type=int, default=10, help="The number of topk")
    args = parser.parse_args()
    return args

if __name__ == "__main__":
    args = arg_parse()
    mmrag = MMRAG(
        model_path=args.model_path, #ì¶”ê°€ í•„ìš”
        dataset=args.dataset,
        query_file=args.query_file,
        experiment_type=args.experiment_type,
        workers_num=args.workers_num,
        topk=args.topk
    )
    mmrag.eval_dataset()