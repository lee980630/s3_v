# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from verl import DataProto
from verl.utils.reward_score import _default_compute_score
import torch
import json
import requests
import math
import numpy as np
import os
import re #added
def dcg(relevance_scores):
    """
    è®¡ç®—æŠ˜æ‰£ç´¯ç§¯å¢ç›Šï¼ˆDCGï¼‰
    :param relevance_scores: ä¸€ä¸ªåˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§åˆ†æ•°
    :return: DCG å€¼
    """
    dcg_value = 0.0
    for i, relevance in enumerate(relevance_scores, start=1):
        dcg_value += (2 ** relevance - 1) / np.log2(i + 1)
    return dcg_value

def ndcg(sorted_docs, golden_answer_list):
    """
    è®¡ç®—å½’ä¸€åŒ–æŠ˜æ‰£ç´¯ç§¯å¢ç›Šï¼ˆNDCGï¼‰
    :param sorted_docs: ä¸€ä¸ªåˆ—è¡¨ï¼Œè¡¨ç¤ºå·²ç»æ’å¥½åºçš„æ–‡æ¡£
    :param golden_answer_list: ä¸€ä¸ªåˆ—è¡¨ï¼Œè¡¨ç¤ºæ‰€æœ‰ç›¸å…³æ–‡æ¡£ï¼ˆgolden answersï¼‰
    :return: NDCG å€¼
    """
    # å°†æ–‡æ¡£æ˜ å°„ä¸ºç›¸å…³æ€§åˆ†æ•°ï¼ˆåœ¨ golden_answer_list ä¸­çš„æ–‡æ¡£ä¸º 1ï¼Œå¦åˆ™ä¸º 0ï¼‰
    relevance_scores = [1 if doc in golden_answer_list else 0 for doc in sorted_docs]
    
    # è®¡ç®— DCG
    dcg_value = dcg(relevance_scores)
    
    # è®¡ç®— IDCGï¼ˆç†æƒ³æƒ…å†µä¸‹çš„ DCGï¼Œæ‰€æœ‰ç›¸å…³æ–‡æ¡£éƒ½æ’åœ¨å‰é¢ï¼‰
    ideal_relevance_scores = [1] * len(golden_answer_list) + [0] * (len(sorted_docs) - len(golden_answer_list))
    idcg_value = dcg(ideal_relevance_scores)
    
    # é˜²æ­¢åˆ†æ¯ä¸ºé›¶
    if idcg_value == 0:
        return 0.0
    
    # è®¡ç®— NDCG
    ndcg_value = dcg_value / idcg_value
    return ndcg_value

def get_answer_from_predict_str(text):
    end_tag = '</answer>'
    start_tag = '<answer>'
    
    end_pos = text.rfind(end_tag)
    if end_pos == -1:
        return None  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°</answer>ï¼Œè¿”å›None
    
    start_pos = text.rfind(start_tag, 0, end_pos)
    if start_pos == -1:
        return None  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°<answer>ï¼Œè¿”å›None
    
    start_pos += len(start_tag)  # è·³è¿‡<answer>æ ‡ç­¾
    return text[start_pos:end_pos]


class RMManager:
    """The reward manager.
      Besides returning token level rewards, this manager records a detailed log
    for each prompt and agent response to facilitate analysis of the GRPO
    training process.
    """

    #def __init__(self, tokenizer, num_examine, compute_score=None,rm_url="http://0.0.0.0:8003/eval") -> None: ìˆ˜ì •:logì‘ì„±
    #ìˆ˜ì • ì¶”ê°€ë³¸#
    def __init__(
        self,
        tokenizer,
        num_examine,
        compute_score=None,
        rm_url="http://0.0.0.0:8003/eval",
        log_path="./logs/grpo_log.json",
    ) -> None:
    #ì¶”ê°€ ë
        self.tokenizer = tokenizer
        self.num_examine = num_examine  # the number of batches of decoded responses to print to the console
        self.compute_score = compute_score or _default_compute_score
        self.rm_url = rm_url
        self.log_path = log_path #ìˆ˜ì • ì¶”ê°€ log ì‘ì„±

    def verify(self, data):
        scores = []
        for i in range(len(data)):
            data_item = data[i]  # DataProtoItem

            prompt_ids = data_item.batch['prompts']

            prompt_length = prompt_ids.shape[-1]

            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()
            valid_prompt_ids = prompt_ids[-valid_prompt_length:]

            response_ids = data_item.batch['responses']
            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum()
            valid_response_ids = response_ids[:valid_response_length]

            # decode
            prompt_str = self.tokenizer.decode(valid_prompt_ids)
            response_str = self.tokenizer.decode(valid_response_ids)

            ground_truth = data_item.non_tensor_batch['reward_model']['ground_truth']

            data_source = data_item.non_tensor_batch['data_source']

            extra_info = data_item.non_tensor_batch.get('extra_info', None)

            raw_score = self.compute_score( 
                data_source=data_source,
                solution_str=response_str,
                ground_truth=ground_truth,
                extra_info=extra_info,
            )
            scores.append(raw_score)
        data.batch['acc'] = torch.tensor(scores, dtype=torch.float32, device=prompt_ids.device)
        return scores

    def __call__(self, data: DataProto):
        """We will expand this function gradually based on the available datasets"""

        # If there is rm score, we directly return rm score. Otherwise, we compute via rm_score_fn
        if 'rm_scores' in data.batch.keys():
            return data.batch['rm_scores']

        #reward_tensorëŠ” ìµœì¢… ì ìˆ˜ë“¤ì„ ë‹´ì„ 'ì„±ì í‘œ'
        reward_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32)

        already_print_data_sources = {}

        #ìˆ˜ì • ì¶”ê°€: log ì‘ì„±#
        if os.path.exists(self.log_path):
            with open(self.log_path, "r") as f:
                try:
                    log_data = json.load(f)
                except json.JSONDecodeError:
                    log_data = {}
        else:
            log_data = {}
        #ìˆ˜ì • ì¶”ê°€ ë

        #ê° ë‹µì•ˆì§€ì—ì„œ 'ë¬¸ì œ', 'í•™ìƒ ë‹µ', 'ì •ë‹µ'ì„ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ì„œ 'ì™¸ë¶€ ì±„ì  ìœ„ì›ì—ê²Œ ë³´ë‚¼ ì„œë¥˜ ë¬¶ìŒ'(data_eval)ì„ ë§Œë“­ë‹ˆë‹¤.
        #data_eval: ëª¨ë“  ë°ì´í„°ì˜ (ì§ˆë¬¸, ìƒì„± ë‹µë³€, ì •ë‹µ) ìŒì´ ë“¤ì–´ìˆëŠ” ë¦¬ìŠ¤íŠ¸.
        data_eval = []
        for i in range(len(data)):
            data_item = data[i]
            prompt_ids = data_item.batch['prompts']
            prompt_length = prompt_ids.shape[-1]
            response_ids = data_item.batch['responses']
            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum()
            valid_response_ids = response_ids[:valid_response_length]
            extra_info = data_item.non_tensor_batch.get('extra_info', None)
            generated_answer = get_answer_from_predict_str(self.tokenizer.decode(valid_response_ids))
            if generated_answer is None:
                generated_answer = 'Please Judge False'
            data_eval.append(dict(
                query = extra_info['question'],
                generated_answer = generated_answer,
                reference_answer = data_item.non_tensor_batch['reward_model']['ground_truth']
            ))

        #data_to_be_eval: data_eval ì¤‘ì—ì„œ raw_scoreê°€ 0ë³´ë‹¤ í° ë°ì´í„°ë§Œ í•„í„°ë§ë˜ì–´ ë“¤ì–´ìˆëŠ” ë¦¬ìŠ¤íŠ¸.
        data_to_be_eval = []
        for i in range(len(data)):
            data_item = data[i]  # DataProtoItem

            prompt_ids = data_item.batch['prompts']

            prompt_length = prompt_ids.shape[-1]

            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()
            valid_prompt_ids = prompt_ids[-valid_prompt_length:]

            response_ids = data_item.batch['responses']
            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum()
            valid_response_ids = response_ids[:valid_response_length]

            # decode
            prompt_str = self.tokenizer.decode(valid_prompt_ids)
            response_str = self.tokenizer.decode(valid_response_ids)

            ground_truth = data_item.non_tensor_batch['reward_model']['ground_truth']

            data_source = data_item.non_tensor_batch['data_source']

            extra_info = data_item.non_tensor_batch.get('extra_info', None)

            #score = self.compute_score( #ìˆ˜ì • ì œê±° log ì‘ì„±
            raw_score = self.compute_score( #ìˆ˜ì • ì¶”ê°€ log ì‘ì„±
                data_source=data_source,
                solution_str=response_str,
                ground_truth=ground_truth,
                extra_info=extra_info,
            )
            
            #if score >0.0: ìˆ˜ì • ì œê±° log ì‘ì„±
            if raw_score > 0.0:#ìˆ˜ì • ì¶”ê°€ log ì‘ì„±
                data_to_be_eval.append(data_eval[i])

        #data_to_be_eval = data_eval #no format ìˆ˜ì • ì œê±° 

        if len(data_to_be_eval) > 0:
            request_data_to_be_eval = dict(
                bs=300,
                prompts=data_to_be_eval
            )
            #ì™¸ë¶€ api ìˆ˜ì • 
            #prompts_json = json.dumps(request_data_to_be_eval) #ìˆ˜ì • ì™¸ë¶€ api ì œê±°
            print("=====================eval model start=====================")
            #response = requests.post(self.rm_url, json=prompts_json) #ì™¸ë¶€ api ìˆ˜ì • ì œê±°
            response = requests.post(self.rm_url, json=request_data_to_be_eval) #ìˆ˜ì • ì¶”ê°€ ì™¸ë¶€ api
            eval_results = response.json()
            print("=====================eval model end=====================")
            ###############3
            
        for i in range(len(data)):
            data_item = data[i]  # DataProtoItem

            prompt_ids = data_item.batch['prompts']

            prompt_length = prompt_ids.shape[-1]

            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()
            valid_prompt_ids = prompt_ids[-valid_prompt_length:]

            response_ids = data_item.batch['responses']
            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum()
            valid_response_ids = response_ids[:valid_response_length]

            # decode
            prompt_str = self.tokenizer.decode(valid_prompt_ids)
            response_str = self.tokenizer.decode(valid_response_ids)

            ground_truth = data_item.non_tensor_batch['reward_model']['ground_truth']

            data_source = data_item.non_tensor_batch['data_source']

            extra_info = data_item.non_tensor_batch.get('extra_info', None)

            raw_score = self.compute_score(
                data_source=data_source,
                solution_str=response_str,
                ground_truth=ground_truth,
                extra_info=extra_info,
            )            
           

            # ###############ìˆ˜ì • (ì‚½ì…) ###########
            # # ì´ìœ : ë‚´ë¶€ ì ìˆ˜ ëŒ€ì‹  API ê²°ê³¼ì™€ NDCG ì ìˆ˜ë§Œìœ¼ë¡œ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
            # model_eval_score = eval_results[i] if i < len(eval_results) else 0.0
            # ndcg_value = 0.0
            
            # # if score > 0.0: # [ì£¼ì„ ì²˜ë¦¬] ë‚´ë¶€ ì ìˆ˜ í•„í„°ë§ì„ ì œê±°í•©ë‹ˆë‹¤.
            # try:
            #     retrievaled_images_basename_list = [os.path.basename(item.rstrip('/')).split(".jpg")[0] for item in data_item.non_tensor_batch['retrievaled_images']]
            #     reference_images_basename_list = [f'{extra_info["file_name"].split(".pdf")[0]}_{page}' for page in extra_info["reference_page"].tolist()]
            #     ndcg_value = ndcg(retrievaled_images_basename_list, reference_images_basename_list)
            # except Exception as e:
            #      # NDCG ê³„ì‚°ì€ RAG ê´€ë ¨ ë°ì´í„°ì—ë§Œ í•´ë‹¹í•˜ë¯€ë¡œ, ì—ëŸ¬ê°€ ë‚˜ë„ ë¬´ì‹œí•˜ê³  ì§„í–‰í•©ë‹ˆë‹¤.
            #     pass

            # score = 0.8 * float(model_eval_score) + 0.2 * ndcg_value
            # #################ìˆ˜ì • ì™„ë£Œ (ì‚½ì…) ###############

            model_eval_score = 0.0
            ndcg_value = 0.0
            final_score = 0.0
            retrievaled_images_basename_list = []
            reference_images_basename_list = []


            ################ìˆ˜ì •(ì£¼ì„ ì²˜ë¦¬) ################    
            #log ì‘ì„±      
            
            retrievaled_images_basename_list = [os.path.basename(item.rstrip('/')).split(".jpg")[0] for item in data_item.non_tensor_batch['retrievaled_images']]
            reference_images_basename_list = [f'{extra_info["file_name"].split(".pdf")[0]}_{page}' for page in extra_info["reference_page"].tolist()]
            
            if raw_score >0.0:    
                ndcg_value = ndcg(retrievaled_images_basename_list, reference_images_basename_list)

                model_eval_score = eval_results.pop(0)
                # score = 0.8*model_eval_score + 0.2*ndcg_value
                final_score = 0.2*model_eval_score + 0.1*raw_score + 0.7*ndcg_value
            ################ìˆ˜ì • ì™„ë£Œ(ì£¼ì„ì²˜ë¦¬) #################

            #ìˆ˜ì • ì¶”ê°€: log ì‘ì„±##


            # # 1. ë³€ìˆ˜ ì´ˆê¸°í™” ì¶”ê°€
            # retrievaled_images_basename_list = []
            # reference_images_basename_list = []            

            # try:
            #     retrievaled_images_basename_list = [os.path.basename(item.rstrip('/')).split(".jpg")[0] for item in data_item.non_tensor_batch['retrievaled_images']]
            #     reference_images_basename_list = [f'{extra_info["file_name"].split(".pdf")[0]}_{page}' for page in extra_info["reference_page"].tolist()]
            #     ndcg_value = ndcg(retrievaled_images_basename_list, reference_images_basename_list)
            # except Exception as e:
            #     # RAG ê´€ë ¨ ë°ì´í„°ê°€ ì•„ë‹ ê²½ìš° NDCG ê³„ì‚°ì—ì„œ ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ 0.0ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            #     ndcg_value = 0.0

            # # raw_score í•„í„°ë§ì´ ì—†ìœ¼ë¯€ë¡œ, ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ model_eval_scoreë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            # model_eval_score = eval_results.pop(0) if eval_results else 0.0
            # final_score = (
            #     0.4 * model_eval_score + 0.6 * ndcg_value # raw_score í•­ì„ ì œê±°í•˜ê³  ê°€ì¤‘ì¹˜ ì¬ë¶„ë°° (0.7, 0.2 -> 0.8, 0.2)
            # )

            reward_tensor[i, valid_response_length - 1] = final_score


            #old logging
            # # structured logging
            # uid = str(data_item.non_tensor_batch['uid'])
            # query_key = uid
            # if query_key not in log_data:
            #     log_data[query_key] = {"prompt": prompt_str, "agents": []}

            # agent_id = len(log_data[query_key]["agents"]) + 1
            # log_data[query_key]["agents"].append(
            #     {
            #         "agent_id": agent_id,
            #         "response": response_str,
            #         "ğŸ“£generated_answerğŸ“£": data_eval[i]['generated_answer'], 
            #         "scores": {
            #             "raw_score": raw_score,                        
            #             "model_eval_score": model_eval_score,
            #             "ndcg_value": ndcg_value,
            #             "â­ï¸final_scoreâ­ï¸": final_score,
            #             "ndcg_details": {
            #                 "retrieved_documents": retrievaled_images_basename_list,
            #                 "reference_documents": reference_images_basename_list,
            #             }

            #         },
            #     }
            # )    
            ####ìˆ˜ì • ì¶”ê°€ ì™„ë£Œ: log ì‘ì„±###        
            retrieved_image_files = [os.path.basename(p) for p in data_item.non_tensor_batch.get('retrievaled_images', [])]
            
            # 2. ë¡œê·¸ì— ê¸°ë¡í•  response ë¬¸ìì—´ì„ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.
            response_str_for_log = response_str
            if retrieved_image_files:
                # ì´ë¯¸ì§€ ê²½ë¡œë¡œ ì±„ì›Œì§„ ë³´ê¸° ì¢‹ì€ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ë§Œë“­ë‹ˆë‹¤.
                image_placeholder = f" [Image Paths: {', '.join(retrieved_image_files)}] "
                # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•´ <|vision_start|>ì™€ <|vision_end|> ì‚¬ì´ì˜ ëª¨ë“  ë‚´ìš©ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ êµì²´í•©ë‹ˆë‹¤.
                response_str_for_log = re.sub(
                    r"(<\|vision_start\|>).*?(<\|vision_end\|>)",
                    r"\1" + image_placeholder + r"\2",
                    response_str,
                    flags=re.DOTALL
                )

            # structured logging
            uid = str(data_item.non_tensor_batch['uid'])
            query_key = uid
            if query_key not in log_data:
                log_data[query_key] = {"prompt": prompt_str, "agents": []}

            agent_id = len(log_data[query_key]["agents"]) + 1
            log_data[query_key]["agents"].append(
                {
                    "agent_id": agent_id,
                    "response": response_str_for_log,  #
                    "ğŸ“£generated_answerğŸ“£": data_eval[i]['generated_answer'], 
                    "scores": {
                        "raw_score": raw_score,                        
                        "model_eval_score": model_eval_score,
                        "ndcg_value": ndcg_value,
                        "â­ï¸final_scoreâ­ï¸": final_score,
                        "ndcg_details": {
                            "retrieved_documents": retrievaled_images_basename_list,
                            "reference_documents": reference_images_basename_list,
                        }

                    },
                }
            )            

            if data_source not in already_print_data_sources:
                already_print_data_sources[data_source] = 0

            if already_print_data_sources[data_source] < self.num_examine:
                already_print_data_sources[data_source] += 1
                print("[prompt]", prompt_str)
                print("[response]", response_str)
                print("[ground_truth]", ground_truth)
                #print("[score]", score) ìˆ˜ì • ì œê±°: log ì‘ì„±
                print("[score]", final_score) #ìˆ˜ì • ì¶”ê°€ : log ì‘ì„±

        ###ìˆ˜ì • ì¶”ê°€:log ì‘ì„±#
        os.makedirs(os.path.dirname(self.log_path), exist_ok=True)
        with open(self.log_path, "w") as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)            
        ###ìˆ˜ì • ì¶”ê°€ ë: log ì‘ì„±#


        return reward_tensor